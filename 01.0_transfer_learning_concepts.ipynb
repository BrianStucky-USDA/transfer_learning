{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9497a54",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src=\"https://github.com/PracticumAI/practicumai.github.io/blob/84b04be083ca02e5c7e92850f9afd391fc48ae2a/images/icons/practicumai_computer_vision.png?raw=true\" alt=\"Practicum AI: Computer Vision icon\" align=\"right\" width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88515b",
   "metadata": {},
   "source": [
    "# Transfer Learning Concepts\n",
    "\n",
    "You may recall *Practicum AI*\"s heroine Amelia, the AI-savvy nutritionist. At the end of our *[Deep Learning Foundations course](https://practicumai.org/courses/deep_learning/)*, Amelia was helping with a computer vision project. If only she had known about transfer learning, it could have saved her a lot of time! In this notebook, we will get some hands-on experience with transfer learning and show you how to use it to improve your workflows.\n",
    "\n",
    "![Figure 2 of the AgriNet paper used as the cover image for this notebook. Figure 2 depicts using transfer learning to make a computer vision model more efficient](images/agrinet_figure-cover.jpg)\n",
    "\n",
    "\n",
    "## AI Pathway review for Transfer Learning & AgriNet \n",
    "\n",
    "If you have taken our [Getting Started with AI course](https://practicumai.org/courses/getting_started/), you may remember this figure of the AI Application Development Pathway. Let's take a quick review of how we will apply this to our case study of AgriNet and it's use of transfer learning.\n",
    "\n",
    "![AI Application Development Pathway image showing the 7 steps in developing an AI application](https://practicumai.org/getting_started/images/application_dev_pathway.png)\n",
    "\n",
    "1. **Choose a problem to solve:** In this example, we will be trying to make a computer vision model that can recognize images of plants, and categorize them as \"healthy\", \"diseased\", and a few other class categories that are plant-specific. \n",
    "2. **Gather data:** The data for the example comes from [HuggingFace](https://www.huggingface.co//), a great repository of datasets, code, and models.\n",
    "3. **Clean and prepare the data:** In the *Deep Learning Foundations* course, we assumed that this was done for us. One issue that we ran into was that of class imbalance. Here, the (probably very tired) researchers that created the AgriNet dataset have already balanced the classes for us!\n",
    "4. **Choose a model:** In the *Deep Learning Foundations* course, we presented the model with little detail. Here, we will use a Convolutional Neural Network (CNN) as our model, trained from scratch as a baseline, and compare it to a pre-trained VGG19 model that we've fine-tuned on the AgriNet dataset. VGG19 (Visual Geometry Group, with 19 layers) is a specific computer vision model architecture, and is pictured in the Cover Image above! We're using a version of VGG19 that has been pre-trained on the ImageNet dataset, which is a large dataset of images with 1000 classes. By fine-tuning this model, we can leverage the knowledge it has learned from the ImageNet dataset and apply it to our specific problem of plant classification. Since the Domain of ImageNet is distinctly different (1000 everyday objects) from the Domain of AgriNet (healthy and diseased plants), we'd say this was a Domain Transfer. \n",
    "   * In the step where you'd choose a model, one can approach this many ways, for this notebook we'll just mention two:\n",
    "      * **Train from scratch:** This is where you start with a randomly initialized model and train it on your data. This can be computationally expensive and time-consuming.\n",
    "      * **Domain Transfer via Fine-Tuning:** This is where you start with a pre-trained model and fine-tune it on your data. This is often faster and requires less data.\n",
    "5. **Train the model:** As mentioned in step 4, we'll demonstrate two approaches in this notebook:\n",
    "      - Training a baseline model from scratch.\n",
    "      - Fine-tuning a VGG19 model pre-trained on ImageNet, a domain-specific dataset to achieve Domain Transfer.\n",
    "6. **Evaluate the model:** We will use the metrics we gather to make decisions about the model. \n",
    "7. **Deploy the model:** We won't get to this stage in this course, but ideally we would end up with a model that could be deployed and achieve relatively good accuracy at solving crop classification problems.\n",
    "\n",
    "\n",
    "### A Quick Primer on the Baseline Model\n",
    "We've trained a simple convolutional neural network (CNN) from scratch as a baseline for comparison. If you have time and want to see how the CNN is set up, check out the [00.5_transfer_learning_helper.ipynb](00.5_transfer_learning_helper.ipynb) notebook that is included in this repository. \n",
    "\n",
    "Strictly speaking, a thorough knowledge of CNNs is not required for this notebook, but if you're interested in learning more, we recommend the our [PracticumAI: Computer Vision](https://github.com/PracticumAI/computer_vision) Intermediate course. That said, with *any* machine learning work, the better you understand the model, the better you can tune it to your needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c096f",
   "metadata": {},
   "source": [
    "### 1. Import the libraries we will use\n",
    "\n",
    "As always, we will start by importing the libraries we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38ea8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import tqdm\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.models import VGG19_Weights\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import ImageFile\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d30e8-55cf-42fb-bf71-e59af2c12a8e",
   "metadata": {},
   "source": [
    "#### Check for GPU availability\n",
    "\n",
    "This cell will check that everything is configured correctly to use your GPU. If everything is correct, you should see something like: \n",
    "\n",
    "    Using GPU: type of GPU\n",
    "\n",
    "If you see:\n",
    "    \n",
    "    Using CPU\n",
    "    \n",
    "Either you do not have a GPU or the kernel is not correctly configured to use it. You might be able to run this notebook, but some sections will take a loooooong time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac0c32-e2f7-43af-bd5b-0e08e7b858b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7859c22",
   "metadata": {},
   "source": [
    "### 2. Getting the data\n",
    "\n",
    "Gotta have data to train, validate and test our models! We will use the AgriNet dataset, which is a dataset of images of 31 crops. For most, there are images of leaves that are healthy or exhibiting one or more dieseases. In total, there are 86 categories of crops and healthy/diseased states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423ecd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the dataset, extract it to the data folder and remove the zip file\n",
    "download_path = \"https://data.rc.ufl.edu/pub/practicum-ai/Transfer_Learning_Intermediate/agrinet_curated.zip\"\n",
    "zip_path = \"data/agrinet_curated.zip\"\n",
    "data_path = \"data\"\n",
    "\n",
    "# Paths to dataset\n",
    "train_dir = os.path.join(data_path, \"agri_net_train\")\n",
    "val_dir = os.path.join(data_path, \"agri_net_val\")\n",
    "test_dir = os.path.join(data_path, \"agri_net_test\")\n",
    "\n",
    "# Check if the data is already loaded\n",
    "if not (\n",
    "    os.path.exists(train_dir) and os.path.exists(val_dir) and os.path.exists(test_dir)\n",
    "):\n",
    "    # Create the data directory if it does not exist\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # Download the zip file\n",
    "    r = requests.get(download_path)\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "\n",
    "    # Remove the zip file\n",
    "    os.remove(zip_path)\n",
    "    print(f'Data has been downloaded an unziped at {data_path}')\n",
    "else:\n",
    "    print(\"Data is already downloaded.\")\n",
    "\n",
    "# Download the models, extract them to the models folder and remove the zip file\n",
    "model_download_path = \"https://data.rc.ufl.edu/pub/practicum-ai/Transfer_Learning_Intermediate/transfer_learning_concepts_models.zip\"\n",
    "model_zip_path = \"models/transfer_learning_concepts_models.zip\"\n",
    "model_data_path = \"models\"\n",
    "\n",
    "# Paths to models\n",
    "baseline_model_trained = os.path.join(model_data_path, \"baseline_model.pt\")\n",
    "vgg19_model_ft = os.path.join(model_data_path, \"vgg19_model.pt\")\n",
    "\n",
    "# Check if the data is already loaded\n",
    "if not (os.path.exists(baseline_model_trained) and os.path.exists(vgg19_model_ft)):\n",
    "    # Create the data directory if it does not exist\n",
    "    if not os.path.exists(model_data_path):\n",
    "        os.makedirs(model_data_path)\n",
    "\n",
    "    # Download the zip file\n",
    "    r = requests.get(model_download_path)\n",
    "    with open(model_zip_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(model_zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(model_data_path)\n",
    "\n",
    "    # Remove the zip file\n",
    "    os.remove(model_zip_path)\n",
    "    print(f'Models been downloaded an unziped at {model_data_path}')\n",
    "else:\n",
    "    print(\"Models are already loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16295c47",
   "metadata": {},
   "source": [
    "### 3. Looking at the Data\n",
    "\n",
    "We will take a look at the data to see what we are working with. This is a good practice to get a sense of the data and to see if there are any issues that need to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c3d80b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(normalize=False):\n",
    "    \"\"\"\n",
    "    Create a transform pipeline with optional normalization\n",
    "    \n",
    "    Args:\n",
    "        normalize (bool): Whether to include normalization for model input\n",
    "    \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: The composed transforms\n",
    "    \"\"\"\n",
    "    # Start with the basic transforms needed for all use cases\n",
    "    transform_list = [\n",
    "        transforms.Resize(256),  # Resize the smaller edge to 256\n",
    "        transforms.CenterCrop(224),  # Crop the center 224x224 pixels\n",
    "        transforms.ToTensor(),  # Convert image to PyTorch Tensor (scales pixels to [0, 1])\n",
    "    ]\n",
    "    \n",
    "    # Add normalization if requested (typically for model input)\n",
    "    if normalize:\n",
    "        transform_list.append(\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        )\n",
    "    \n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "# For visualization (no normalization)\n",
    "vis_transforms = get_transforms(normalize=False)\n",
    "\n",
    "# For model input (with normalization)\n",
    "model_transforms = get_transforms(normalize=True)\n",
    "\n",
    "# Load the test dataset using the visualization transforms\n",
    "test_dataset = datasets.ImageFolder(test_dir, vis_transforms)\n",
    "\n",
    "# Get class names from the dataset folders\n",
    "class_names = test_dataset.classes\n",
    "print(f\"Found classes: {class_names}\")\n",
    "\n",
    "# Get the total number of images in the training set\n",
    "num_test_images = len(test_dataset)\n",
    "print(f\"Number of training images: {num_test_images}\")\n",
    "\n",
    "# Don't mind this little guy, it'll help us out later at the end of the notebook\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=False\n",
    ")  # Adjust batch_size as needed\n",
    "\n",
    "# Select 9 random indices\n",
    "random_indices = random.sample(range(num_test_images), 9)\n",
    "\n",
    "# --- Display the images ---\n",
    "\n",
    "plt.figure(figsize=(10, 10))  # Adjust figure size as needed\n",
    "plt.suptitle(\n",
    "    \"9 Random Images from the Training Set\", fontsize=16\n",
    ")  # Add a title to the figure\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "\n",
    "    # Get the image and label from the dataset using the random index\n",
    "    image_tensor, label_index = test_dataset[idx]\n",
    "\n",
    "    # Image tensors from ToTensor() are CxHxW and values are [0, 1].\n",
    "    # Matplotlib expects HxWxC and values [0, 1] for floats or [0, 255] for integers.\n",
    "    # We need to rearrange dimensions using permute.\n",
    "    image_for_plot = image_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image_for_plot)\n",
    "\n",
    "    # Get the class name using the label index\n",
    "    label_name = class_names[label_index]\n",
    "    plt.title(f\"Label: {label_name}\")\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to prevent title overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e1ed6",
   "metadata": {},
   "source": [
    "### 4.1 Examine the Model: Baseline CNN\n",
    "\n",
    "We will look at a sample CNN architecture (the same architecture used to train the baseline model) to see how it is set up. In this notebook we will *not* train the model as that would take a long time (about 10 minutes per epoch on a GPU). After we look at this and the VGG19 model, we'll compare the two using training runs that we've already done.\n",
    "\n",
    "When you have time, the notebook [00.5_transfer_learning_helper.ipynb](00.5_transfer_learning_helper.ipynb) is included in this repository and you can train the model from scratch there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1c486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define baseline model using PyTorch\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 112 * 112, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the model\n",
    "num_classes = 86\n",
    "\n",
    "# Load the model\n",
    "baseline_model = BaselineModel(num_classes)\n",
    "\n",
    "# Display the model architecture\n",
    "print(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b28907-5eb5-4784-a17f-35e85bb76104",
   "metadata": {},
   "source": [
    "Overall, this model is a relatively simple CNN model that you might start with if you built a model from scratch. Again, rather than train this model in this notebook, we are loading in the result of training it. We have saved the trained weights, and are loading these in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1760f",
   "metadata": {},
   "source": [
    "### 4.2 Examine the Model: VGG19\n",
    "\n",
    "We will look at the VGG19 model architecture to see how it is set up. This model was a state of the art model in 2014, winning the ImageNet competion. You can check out the [Simonyan and Zisserman (2014) paper on the VGG models here](https://arxiv.org/abs/1409.1556)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927ac76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the base model VGG19 architecture from torchvision\n",
    "vgg19 = models.vgg19(pretrained=False)  # Start with non-pretrained model\n",
    "\n",
    "# Modify the final classifier layer to match your fine-tuned model (86 classes)\n",
    "vgg19.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "# Now load the weights for VGG19 that we fine-tuned\n",
    "vgg19.load_state_dict(torch.load(vgg19_model_ft))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "vgg19.eval()\n",
    "\n",
    "# Display the model architecture\n",
    "print(vgg19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd739485",
   "metadata": {},
   "source": [
    "### 5. Compare the Models\n",
    "\n",
    "Even if you're unfamiliar with the ins and outs of computer vision, it's easy to see from the prints of the architectures that VGG19 is a lot more complex than the simple CNN we've set up. Given that VGG19 is much larger, it would normally be the case that training it from random weights would take a lot longer. However, because it's pre-trained on ImageNet, we can *fine-tune* just the last few layers to get a model that is more accurate than the baseline CNN, and in comparable time!\n",
    "\n",
    "One measure of model complexity is the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01a975-fe40-4254-98fd-dbee08a5876d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    '''Count the number of parameters in a model.'''\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print parameter numbers\n",
    "\n",
    "print(f\"The 'baseline_model' model has {count_parameters(baseline_model):,} parameters\")\n",
    "print(f\"The 'vgg19' model has {count_parameters(vgg19):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af825b56-01d4-40b1-b1e4-dfa2e91fd95a",
   "metadata": {},
   "source": [
    "### 6. Evaluate the Models\n",
    "\n",
    "Earlier in the notebook we downloaded trained models: \n",
    "\n",
    "1. The `baseline_model` was trained from scratch. It took approximately 50 minutes to train for 5 epochs.\n",
    "1. The `vgg19` was fine-tuned from the VGG19 model starting from the weights learned in training on the ImageNet dataset. It took roughly the same amount of time (50 minutes for 5 epochs), even though it has nearly 3 times more parameters.\n",
    "\n",
    "We will load these models and evaluate them on the test set to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c5135-1bfb-43da-a7c5-8a9e0b989357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Allow loading of truncated images, since the dataset's images aren't all the same size!\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def pil_loader(path):\n",
    "    \"\"\"Custom image loader that handles truncated images\"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a dataset with progress bar\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to evaluate\n",
    "        data_loader: DataLoader containing the evaluation data\n",
    "        criterion: Loss function\n",
    "        device: Device to run evaluation on (cuda/cpu)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (loss, accuracy) on the dataset\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode and move to device\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize variables to track metrics\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    \n",
    "    # Iterate over the dataset with progress bar\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm.tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "                \n",
    "            # Update loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects.double() / total_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Load our trained Baseline CNN model\n",
    "baseline_model.load_state_dict(torch.load(baseline_model_trained))\n",
    "\n",
    "# Create a DataLoader with custom image loader to handle truncated images\n",
    "test_dataset = datasets.ImageFolder(test_dir, model_transforms, loader=pil_loader)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate the baseline model\n",
    "baseline_loss, baseline_acc = evaluate_model(baseline_model, test_loader, criterion, device)\n",
    "print(f\"Baseline Model: Loss: {baseline_loss:.4f}, Accuracy: {baseline_acc:.4f}\")\n",
    "\n",
    "# Evaluate the VGG19 model\n",
    "vgg19_loss, vgg19_acc = evaluate_model(vgg19, test_loader, criterion, device)\n",
    "print(f\"VGG19 Model: Loss: {vgg19_loss:.4f}, Accuracy: {vgg19_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1314e02",
   "metadata": {},
   "source": [
    "### 7. Conclusion\n",
    "\n",
    "We can see why the researchers behind AgriNet chose to use transfer learning to train their model. The VGG19 model, pre-trained on ImageNet, was able to achieve a higher accuracy than the CNN trained from scratch. This is a great example of how transfer learning can be used to improve the efficiency of training a model.\n",
    "\n",
    "### Key Takeaways\n",
    "- Fine-tuning is a powerful technique that can be used to improve the efficiency of training a model.\n",
    "- Fine-tuning is especially useful when working with limited data.\n",
    "- Pre-trained models can be fine-tuned on a new dataset to typically achieve better performance than training a model from scratch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
