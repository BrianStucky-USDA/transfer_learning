{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src=\"https://github.com/PracticumAI/practicumai.github.io/blob/84b04be083ca02e5c7e92850f9afd391fc48ae2a/images/icons/practicumai_computer_vision.png?raw=true\" alt=\"Practicum AI: Computer Vision icon\" align=\"right\" width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Implementation\n",
    "\n",
    "Welcome back! In our previous exercise [01.0_transfer_learning_conepts.ipynb](01.0_transfer_learning_conepts.ipynb), we introduced the core concepts of **Transfer Learning** and experimented with fine-tuning. We saw how leveraging pre-trained models can significantly boost performance and reduce training time compared to starting from scratch.\n",
    "\n",
    "In this notebook we'll expand into adapting powerful pre-trained language models for specific Natural Language Processing (NLP) tasks, focusing on two other, important transfer learning strategies. We'll:\n",
    "\n",
    "* Implement **Feature Extraction** using a pre-trained Transformer model for text classification.\n",
    "* Look closer at how to freeze the base model and train only a new classification head.\n",
    "* Implement **LoRA (Low-Rank Adaptation)**, a Parameter-Efficient Fine-Tuning (PEFT) technique, to adapt the same pre-trained Transformer.\n",
    "* Understand how LoRA modifies the model and drastically reduces the number of trainable parameters.\n",
    "* Directly compare the results and efficiency of Feature Extraction versus LoRA on the same NLP task.\n",
    "\n",
    "## A Direct Comparison\n",
    "\n",
    "To make the comparison between Feature Extraction and LoRA as clear as possible, we will:\n",
    "\n",
    "1.  Use the **same base pre-trained model** [DistilBERT - `distilbert-base-uncased`](https://huggingface.co/distilbert/distilbert-base-uncased) as the starting point for both methods.\n",
    "2.  Apply both techniques to the **same text classification task** (e.g., Text Classification using the [Crop Market News Classification](https://www.kaggle.com/datasets/mcwemzy/crop-market-news-classification)). This dataset is hosted on the fantasic model zoo site, [Kaggle.com](kaggle.com)!\n",
    "3.  Use the **same dataset** for training and evaluation in both parts.\n",
    "\n",
    "This setup will allow us to directly observe the differences in implementation complexity, performance metrics, and the number of parameters trained.\n",
    "\n",
    "### Prerequisites and Setup\n",
    "\n",
    "* **Conceptual Understanding:** Ensure you're comfortable with the basic ideas of transfer learning, pre-trained models, and fine-tuning as covered in Notebook `01.0`.\n",
    "* **Helper Functions:** We will utilize helper functions defined in [00.5_transfer_learning_helper.ipynb](00.5_transfer_learning_helper.ipynb). Please make sure you have access to that notebook or have run it previously to make the functions available.\n",
    "* **Deep Learning Fundamentals:** As with the previous exercise a throrough understanding of how Large Language Models (LLMs) work is not required, but it is necessary to have a basic understanding of concepts like parameters, hyperparameters, and other Deep Learning fundamentals. If you do want to learn more about how LLMs work, we recommend NVidia's [Deep Learning Institute](https://developer.nvidia.com/dli) course on [Introduction to Transformer-Based Natural Language Processing](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-08+V1). This course is free and provides a solid foundation in the principles of Transformers and their applications in NLP.\n",
    "\n",
    "## How to Use This Notebook (`FIX_ME`s)\n",
    "\n",
    "In this notebook, you'll find sections marked with:\n",
    "\n",
    "```\n",
    "FIX_ME\n",
    "# FIX_ME: <description of what to do>\n",
    "```\n",
    "These are places where you need to fill in missing code or make adjustments. The goal is to reinforce the key implementation steps for each technique.\n",
    "\n",
    "If you get stuck, review the preceding explanations, check the documentation for the libraries used (PyTorch, Hugging Face), or consult the course's **GitHub Discussions page** [Link to GitHub Discussions - Placeholder] for hints and help from peers and instructors.\n",
    "\n",
    "Let's get started with adapting our language model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the libraries we will use\n",
    "\n",
    "As always, we will start by importing the libraries we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import datasets\n",
    "import peft\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import pandas as pd\n",
    "import arff\n",
    "from tqdm import tqdm\n",
    "from scipy.io import arff as scipy_arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    matthews_corrcoef,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for GPU availability\n",
    "\n",
    "This cell will check that everything is configured correctly to use your GPU. If everything is correct, you should see something like: \n",
    "\n",
    "    Using GPU: type of GPU\n",
    "\n",
    "If you see:\n",
    "    \n",
    "    Using CPU\n",
    "    \n",
    "Either you do not have a GPU or the kernel is not correctly configured to use it. You might be able to run this notebook, but some sections will take a loooooong time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting the data\n",
    "\n",
    "Today's dataset is the [Crop Market News Classification](https://www.kaggle.com/datasets/mcwemzy/crop-market-news-classification) dataset. This dataset contains crop market news articles and their corresponding categories. The goal is to classify the articles into their respective categories using both Feature Extraction and LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset, extract it to the data folder and remove the zip file\n",
    "download_path = \"https://data.rc.ufl.edu/pub/practicum-ai/Transfer_Learning_Intermediate/crop_market_news.zip\"\n",
    "zip_path = \"data/crop_market_news.zip\"\n",
    "data_path = \"data\"\n",
    "\n",
    "# Paths to dataset - UPDATED to use the correct filename\n",
    "dataset = os.path.join(data_path, \"Crop.Market.News.Classification.arff\")\n",
    "\n",
    "# Check if the data is already loaded\n",
    "if not (os.path.exists(dataset) and os.path.getsize(dataset) > 0):\n",
    "    # Create the data directory if it does not exist\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # Download the zip file\n",
    "    r = requests.get(download_path)\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "\n",
    "    # Remove the zip file\n",
    "    os.remove(zip_path)\n",
    "    print(f\"Data has been downloaded an unziped at {data_path}\")\n",
    "else:\n",
    "    print(\"Data is already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Looking at the data\n",
    "\n",
    "The dataset contains a number of columns, but we will only use the `text` and `label` columns. The `text` column contains the news articles, and the `label` column contains the corresponding categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Check if the dataset file exists and is not empty\n",
    "try:\n",
    "    # Use liac-arff to load the ARFF file\n",
    "    with open(dataset, \"r\") as f:\n",
    "        arff_data = arff.load(f)\n",
    "        data = pd.DataFrame(\n",
    "            arff_data[\"data\"], columns=[attr[0] for attr in arff_data[\"attributes\"]]\n",
    "        )\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset file not found at {dataset}.\")\n",
    "    print(\"Please ensure you have downloaded the dataset and the path is correct.\")\n",
    "    data = None  # Set data to None if file not found\n",
    "\n",
    "if data is not None:\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Loaded DataFrame shape: {df.shape}\")\n",
    "\n",
    "    # Decode byte strings (Common in ARFF)\n",
    "    # Identify potential text columns (adjust names based on actual columns in meta/df.info())\n",
    "    text_col = \"text\"\n",
    "    label_col = \"category\"\n",
    "\n",
    "    if text_col in df.columns and df[text_col].dtype == \"object\":\n",
    "        # Check if decoding is needed (inspect first element)\n",
    "        if isinstance(df[text_col].iloc[0], bytes):\n",
    "            print(f\"Decoding byte strings in column '{text_col}'...\")\n",
    "            df[text_col] = df[text_col].str.decode(\"utf-8\")\n",
    "\n",
    "    if label_col in df.columns and df[label_col].dtype == \"object\":\n",
    "        if isinstance(df[label_col].iloc[0], bytes):\n",
    "            print(f\"Decoding byte strings in column '{label_col}'...\")\n",
    "            df[label_col] = df[label_col].str.decode(\"utf-8\")\n",
    "\n",
    "    # Map String Labels to Integer IDs\n",
    "    unique_labels = df[label_col].unique()\n",
    "    num_labels = len(unique_labels)\n",
    "\n",
    "    # Create mappings\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "    # Apply mapping to create a new 'label' column\n",
    "    df[\"label\"] = df[label_col].map(label2id)\n",
    "\n",
    "    print(f\"Number of classes: {num_labels}\")\n",
    "    print(\"Label mappings created:\")\n",
    "    print(f\"  label2id: {label2id}\")\n",
    "    print(f\"  id2label: {id2label}\")\n",
    "\n",
    "    # Inspect the DataFrame\n",
    "    print(\"\\nDataFrame Head:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    df.info()\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    print(df[\"label\"].value_counts())\n",
    "\n",
    "    # Keep only relevant columns (text and integer label)\n",
    "    df = df[[text_col, \"label\"]]\n",
    "    df = df.rename(columns={text_col: \"text\"})  # Ensure text column is named 'text'\n",
    "else:\n",
    "    print(\"Skipping DataFrame processing as data was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preprocessing the data\n",
    "\n",
    "Now, let's convert our Pandas DataFrame into the Hugging Face `datasets` format, which integrates seamlessly with the `transformers` library. Since the original dataset doesn't specify train/test splits, we'll create our own train and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    # Convert Pandas DataFrame to Hugging Face Dataset\n",
    "    hf_dataset = datasets.Dataset.from_pandas(df)\n",
    "    print(\"\\nConverted to Hugging Face Dataset:\")\n",
    "    print(hf_dataset)\n",
    "\n",
    "    # Split into training and validation sets (e.g., 80% train, 20% validation)\n",
    "    train_test_split_ratio = 0.20\n",
    "    dataset_dict = hf_dataset.train_test_split(\n",
    "        test_size=train_test_split_ratio, shuffle=True, seed=42\n",
    "    )  # Use seed for reproducibility\n",
    "\n",
    "    # Rename for clarity\n",
    "    train_ds = dataset_dict[\"train\"]\n",
    "    eval_ds = dataset_dict[\"test\"]\n",
    "\n",
    "    print(\"\\nSplit into Train and Validation Sets:\")\n",
    "    print(f\"  Training examples: {len(train_ds)}\")\n",
    "    print(f\"  Validation examples: {len(eval_ds)}\")\n",
    "    print(train_ds)  # Show structure\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    model_name = \"bert-base-uncased\"  # Change to your preferred model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(\n",
    "            example[\"text\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "        )\n",
    "\n",
    "    train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "    eval_ds = eval_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    eval_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "else:\n",
    "    print(\"Skipping Dataset preparation as data was not loaded.\")\n",
    "    train_ds, eval_ds = None, None  # Set to None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Set up the model and tokenizer\n",
    "\n",
    "We will use the `distilbert-base-uncased` model and tokenizer from Hugging Face. This model is a smaller, faster, and lighter version of BERT *(Bidirectional Encoder Representations from Transformers)* that retains 97% of BERT's language understanding while being 60% faster and 40% smaller. It is a great choice for text classification tasks, especially when computational resources are limited. The `distilbert-base-uncased` model is pre-trained on a large corpus of English text and is designed to handle uncased text (i.e., it does not differentiate between uppercase and lowercase letters), making it ideal for our task. We will also set up the training arguments for both Feature Extraction and LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up models for LoRA and Feature Extraction\n",
    "\n",
    "# --- LoRA Setup ---\n",
    "# Load base model for LoRA\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Adjust based on your classification task\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of the update matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"query\", \"key\", \"value\"],  # Which modules to apply LoRA to\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,  # For sequence classification tasks\n",
    ")\n",
    "\n",
    "# Create LoRA model\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "print(\n",
    "    f\"LoRA model setup complete. Trainable parameters: {sum(p.numel() for p in lora_model.parameters() if p.requires_grad)}\"\n",
    ")\n",
    "\n",
    "# --- Feature Extraction Setup ---\n",
    "# Load the base model for feature extraction\n",
    "feature_extractor_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Adjust based on your classification task\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in feature_extractor_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze just the classification head for fine-tuning\n",
    "for param in feature_extractor_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Create dataloaders\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds, shuffle=True, batch_size=16, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_ds, batch_size=16, collate_fn=data_collator)\n",
    "\n",
    "print(\n",
    "    f\"Feature extraction model setup complete. Trainable parameters: {sum(p.numel() for p in feature_extractor_model.parameters() if p.requires_grad)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training the models\n",
    "\n",
    "We will train two models: one using Feature Extraction and the other using LoRA. We will use the same training arguments for both models to ensure a fair comparison. The main difference will be in the training process: \n",
    "A. For Feature Extraction, we will freeze the base model and train only the classification head.\n",
    "B. For LoRA, we will use the LoRA technique to adapt the entire model. \n",
    "\n",
    "We will evaluate both models on the validation set and compare their performance in the section 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            references.extend(labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(references, predictions)\n",
    "    f1 = f1_score(references, predictions, average=\"weighted\")\n",
    "\n",
    "    return {\"loss\": total_loss / len(dataloader), \"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, eval_dataloader, num_epochs=3, device=\"cuda\"):\n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=5e-5)\n",
    "\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    results = {\"train_loss\": [], \"eval_loss\": [], \"eval_accuracy\": [], \"eval_f1\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Training\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        # Evaluation\n",
    "        eval_metrics = evaluate(model, eval_dataloader, device)\n",
    "        results[\"eval_loss\"].append(eval_metrics[\"loss\"])\n",
    "        results[\"eval_accuracy\"].append(eval_metrics[\"accuracy\"])\n",
    "        results[\"eval_f1\"].append(eval_metrics[\"f1\"])\n",
    "\n",
    "        print(\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            + f\"Eval Loss: {eval_metrics['loss']:.4f} | \"\n",
    "            + f\"Accuracy: {eval_metrics['accuracy']:.4f} | \"\n",
    "            + f\"F1 Score: {eval_metrics['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Executing the training\n",
    "\n",
    "We've broken out the training process into a separate function to keep the code clean and organized, and to make it easier to change the hyperparameters later if one was so inclined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA training\n",
    "print(\"Training LoRA model...\")\n",
    "lora_results = train_model(\n",
    "    lora_model, train_dataloader, eval_dataloader, num_epochs=10, device=device\n",
    ")\n",
    "print(\"\\nLoRA training completed!\")\n",
    "\n",
    "print(\"\\nTraining feature extraction model...\")\n",
    "# Feature extraction training\n",
    "feature_extraction_results = train_model(\n",
    "    feature_extractor_model,\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    num_epochs=10,\n",
    "    device=device,\n",
    ")\n",
    "print(\"\\nFeature extraction training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluating the models\n",
    "\n",
    "After training both models, we will evaluate their performance on the validation set. We will compare the accuracy, loss, and examine how the training process proceeded with respect to each approach's trainable parameters. This will help us understand the trade-offs between Feature Extraction and LoRA in terms of performance and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluate the models\n",
    "def get_comprehensive_metrics(y_true, y_pred, y_scores=None):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics for model evaluation\n",
    "\n",
    "    Parameters:\n",
    "    y_true: Ground truth labels\n",
    "    y_pred: Predicted labels\n",
    "    y_scores: Prediction probabilities (for ROC AUC)\n",
    "    \"\"\"\n",
    "    # Basic classification metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    # ROC AUC (if probabilities available)\n",
    "    auc = None\n",
    "    if y_scores is not None:\n",
    "        # For binary classification\n",
    "        if y_scores.shape[1] == 2:\n",
    "            auc = roc_auc_score(y_true, y_scores[:, 1])\n",
    "        # For multi-class\n",
    "        else:\n",
    "            auc = roc_auc_score(y_true, y_scores, multi_class=\"ovr\", average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_model_comparison_bars(lora_metrics, fe_metrics):\n",
    "    \"\"\"Create bar chart comparing LoRA and Feature Extraction metrics\"\"\"\n",
    "    metrics = list(lora_metrics.keys())\n",
    "\n",
    "    # Filter out None values\n",
    "    valid_metrics = [\n",
    "        m for m in metrics if lora_metrics[m] is not None and fe_metrics[m] is not None\n",
    "    ]\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    x = np.arange(len(valid_metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    # Plot bars\n",
    "    ax.bar(x - width / 2, [lora_metrics[m] for m in valid_metrics], width, label=\"LoRA\")\n",
    "    ax.bar(\n",
    "        x + width / 2,\n",
    "        [fe_metrics[m] for m in valid_metrics],\n",
    "        width,\n",
    "        label=\"Feature Extraction\",\n",
    "    )\n",
    "\n",
    "    # Add labels and formatting\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Model Performance Comparison\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(valid_metrics)\n",
    "    ax.legend()\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate([lora_metrics[m] for m in valid_metrics]):\n",
    "        ax.text(i - width / 2, v + 0.01, f\"{v:.4f}\", ha=\"center\", fontsize=9)\n",
    "    for i, v in enumerate([fe_metrics[m] for m in valid_metrics]):\n",
    "        ax.text(i + width / 2, v + 0.01, f\"{v:.4f}\", ha=\"center\", fontsize=9)\n",
    "\n",
    "    plt.ylim(0, 1.1)  # Ensure there's space for labels\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def compare_models_learning_curves(results_dict, metric_names=None):\n",
    "    \"\"\"Compare different models based on their evaluation metrics\"\"\"\n",
    "    if metric_names is None:\n",
    "        metric_names = [\"eval_accuracy\", \"eval_f1\"]\n",
    "\n",
    "    # Create a figure with subplots for each metric\n",
    "    fig, axes = plt.subplots(len(metric_names), 1, figsize=(12, 4 * len(metric_names)))\n",
    "    if len(metric_names) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, metric in enumerate(metric_names):\n",
    "        for model_name, results in results_dict.items():\n",
    "            if metric in results:\n",
    "                axes[i].plot(results[metric], marker=\"o\", label=f\"{model_name}\")\n",
    "\n",
    "        axes[i].set_title(f\"{metric} across epochs\")\n",
    "        axes[i].set_xlabel(\"Epoch\")\n",
    "        axes[i].set_ylabel(metric)\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def evaluate_and_compare(\n",
    "    lora_model, feature_extraction_model, test_dataloader, device, id2label=None\n",
    "):\n",
    "    # Get predictions\n",
    "    lora_preds, lora_true, lora_scores, lora_time = get_predictions(\n",
    "        lora_model, test_dataloader, device\n",
    "    )\n",
    "    fe_preds, fe_true, fe_scores, fe_time = get_predictions(\n",
    "        feature_extraction_model, test_dataloader, device\n",
    "    )\n",
    "\n",
    "    # Get metrics\n",
    "    lora_metrics = get_comprehensive_metrics(lora_true, lora_preds, lora_scores)\n",
    "    fe_metrics = get_comprehensive_metrics(fe_true, fe_preds, fe_scores)\n",
    "\n",
    "    # Print comparison table\n",
    "    print(f\"{'Metric':<15} {'LoRA':<10} {'Feature Extraction':<20}\")\n",
    "    print(\"=\" * 45)\n",
    "    for metric in lora_metrics.keys():\n",
    "        if lora_metrics[metric] is not None and fe_metrics[metric] is not None:\n",
    "            print(f\"{metric:<15} {lora_metrics[metric]:.4f}   {fe_metrics[metric]:.4f}\")\n",
    "\n",
    "    # Print efficiency metrics\n",
    "    print(f\"\\nInference time (s):\")\n",
    "    print(f\"LoRA: {lora_time:.4f}\")\n",
    "    print(f\"Feature Extraction: {fe_time:.4f}\")\n",
    "\n",
    "    # Count trainable parameters\n",
    "    lora_trainable = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "    fe_trainable = sum(\n",
    "        p.numel() for p in feature_extraction_model.parameters() if p.requires_grad\n",
    "    )\n",
    "    print(f\"\\nTrainable parameters:\")\n",
    "    print(f\"LoRA: {lora_trainable:,}\")\n",
    "    print(f\"Feature Extraction: {fe_trainable:,}\")\n",
    "\n",
    "    # Plot comparison bar chart\n",
    "    plt.figure(1)\n",
    "    plot_model_comparison_bars(lora_metrics, fe_metrics)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot confusion matrices with class labels\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Get class labels for confusion matrix\n",
    "    labels = None\n",
    "    if id2label:\n",
    "        labels = [id2label[i] for i in range(len(id2label))]\n",
    "\n",
    "    # Create confusion matrices with labels\n",
    "    cm1 = ConfusionMatrixDisplay.from_predictions(\n",
    "        lora_true,\n",
    "        lora_preds,\n",
    "        ax=ax1,\n",
    "        normalize=\"true\",\n",
    "        display_labels=labels if labels else None,\n",
    "    )\n",
    "    ax1.set_title(\"LoRA Confusion Matrix\")\n",
    "    ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    cm2 = ConfusionMatrixDisplay.from_predictions(\n",
    "        fe_true,\n",
    "        fe_preds,\n",
    "        ax=ax2,\n",
    "        normalize=\"true\",\n",
    "        display_labels=labels if labels else None,\n",
    "    )\n",
    "    ax2.set_title(\"Feature Extraction Confusion Matrix\")\n",
    "    ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return metrics for further analysis if needed\n",
    "    return {\"lora\": lora_metrics, \"feature_extraction\": fe_metrics}\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader, device):\n",
    "    \"\"\"Get predictions, true labels, and measure inference time\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    scores = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels)\n",
    "            scores.append(probs)\n",
    "\n",
    "    inference_time = time.time() - start_time\n",
    "    return (\n",
    "        np.array(predictions),\n",
    "        np.array(true_labels),\n",
    "        np.vstack(scores),\n",
    "        inference_time,\n",
    "    )\n",
    "\n",
    "\n",
    "# Plot learning curves for both models\n",
    "def plot_learning_curves(lora_results, fe_results):\n",
    "    # Combine results for comparison\n",
    "    results_dict = {\"LoRA\": lora_results, \"Feature Extraction\": fe_results}\n",
    "\n",
    "    # Plot learning curves\n",
    "    fig = compare_models_learning_curves(\n",
    "        results_dict, [\"train_loss\", \"eval_loss\", \"eval_accuracy\", \"eval_f1\"]\n",
    "    )\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Evaluate and compare models\n",
    "if train_ds is not None and eval_ds is not None:\n",
    "    print(\"Evaluating and comparing models...\")\n",
    "    metrics = evaluate_and_compare(\n",
    "        lora_model, feature_extractor_model, eval_dataloader, device, id2label\n",
    "    )\n",
    "\n",
    "    # Plot learning curves\n",
    "    print(\"\\nPlotting learning curves...\")\n",
    "    plot_learning_curves(lora_results, feature_extraction_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down the graphs\n",
    "\n",
    "If your stats are a little rusty, you might need a bit of refresher to make sense of the graphs above. Expand the section below for explanations of the terms used above.\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary>Click to expand for stats terms!</summary>\n",
    "\n",
    "<p>\n",
    "\n",
    "##### What is Precision?\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. An example in our fruit object detection task would be the ratio of correctly predicted apples to the total predicted apples. Higher precision values are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is Recall?\n",
    "Recall is the ratio of correctly predicted positive observations to the true number of observations of a class. An example in our fruit object detection task would be the ratio of correctly predicted apples to the total actual apples. Higher recall values are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is Confidence?\n",
    "Confidence is the probability that a model assigns to a prediction. In our fruit object detection task, the confidence is the probability that a model assigns to a fruit being an apple, orange, or any other fruit. Higher confidence values are better, as they indicate that the model is more certain about its predictions.\n",
    "\n",
    "##### What is a confusion matrix?\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made.\n",
    "\n",
    "##### What is the F1 Confidence Curve?\n",
    "The F1 Confidence Curve is a plot of the F1 score against the confidence threshold. The F1 score is the harmonic mean of precision and recall, and it is a measure of a model's accuracy. The confidence threshold is the minimum confidence level that a model must have in order to make a prediction. The F1 Confidence Curve is used to determine the optimal confidence threshold for a model. The curve shows how the F1 score changes as the confidence threshold is varied. The goal is to find the confidence threshold that maximizes the F1 score. The highest point on the curve tells you the optimal confidence threshold where the model strikes the best balance between precision and recall. In our fruit object detection task, the F1 Confidence Curve would show how the F1 score changes as the confidence threshold is varied for each fruit class. Higher F1 scores are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is AUC?\n",
    "AUC stands for Area Under the Curve. It is a performance measurement for classification problems at various threshold settings. AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher AUC values are better, as they indicate that the model is better at distinguishing between classes.\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "#### A detailed comparison\n",
    "\n",
    "What are our metrics actually telling us? In this experiment, the LoRA model significantly outperformed the Feature Extraction model in terms of classification accuracy and ability to learn the task, despite having vastly more trainable parameters. However, the Feature Extraction model was slightly faster during inference.\n",
    "\n",
    "##### Performance Metrics (Accuracy, Precision, F1, AUC):\n",
    "\n",
    "LoRA Wins: LoRA shows notably better scores across the board (Accuracy: ~48% vs 45%, Precision: ~50% vs 21%, F1: ~44% vs 28%, AUC: ~0.66 vs 0.49).\n",
    "Feature Extraction Struggles: Feature Extraction's performance is quite poor, especially its Precision (0.2066) and AUC (0.4896). An AUC below 0.5 suggests the model is performing worse than random guessing at distinguishing between classes. The UndefinedMetricWarning indicates it completely failed to predict certain classes, heavily impacting the precision and F1 scores.\n",
    "The pre-trained features from the frozen base model (used in Feature Extraction) were likely not sufficient or well-suited for this specific classification task. Training only the small classifier head wasn't enough to adapt. LoRA, by adapting parameters within the main Transformer blocks, was able to learn task-specific patterns more effectively, leading to better discrimination between classes.\n",
    "\n",
    "##### Trainable Parameters:\n",
    "\n",
    "There's a massive difference: LoRA trained 886,274 parameters, while Feature Extraction only trained 1,538.\n",
    "This highlights the core difference in the techniques. Feature Extraction is extremely parameter-efficient during training as it only updates the final layer. LoRA injects trainable low-rank matrices into the model, resulting in significantly more parameters to train compared to Feature Extraction, but still vastly fewer than full fine-tuning the entire base model. In this case, the extra trainable parameters used by LoRA were crucial for achieving better performance.\n",
    "\n",
    "##### Inference Time:\n",
    "\n",
    "Feature Extraction Faster: Feature Extraction was slightly faster (0.0236s vs 0.0381s).\n",
    "This is expected. LoRA adds computational steps through its adapter matrices during the forward pass, slightly increasing inference time compared to just running the base model and a simple classifier. The difference here is small, but it could be more significant with larger models or longer sequences.\n",
    "\n",
    "##### Learning Curves:\n",
    "\n",
    "LoRA Learned: LoRA's curves improve quite a bit, which aligns perfectly with its better evaluation metrics. It shows the model was successfully learning and reducing loss/improving accuracy over the training epochs.\n",
    "Feature Extraction Stagnated: The pretty flat curves for Feature Extraction confirm its inability to learn effectively from this data just by training the classifier. The model likely plateaued very early in training.\n",
    "\n",
    "For this specific dataset and task, Feature Extraction proved inadequate. While extremely lightweight in terms of trainable parameters and slightly faster at inference, it failed to learn effectively, resulting in poor performance. LoRA provided a much better trade-off. Although it involved training significantly more parameters (though still far fewer than full fine-tuning) and had slightly slower inference, it successfully adapted the base model to the task, leading to substantially better classification performance and demonstrating clear learning during training. The results suggest the need for adapting more than just the final layer for this particular problem.\n",
    "\n",
    "##### Is Feature Extraction Just Bad?\n",
    "\n",
    "Feature Extraction might be preferable to LoRA when the downstream task is extremely similar to the model's pre-training objective, meaning the existing features require very little adaptation, and computational resources for training are exceptionally limited. In such cases, the simplicity and minimal training cost of Feature Extraction could outweigh the potential performance gains from LoRA's more complex adaptation.\n",
    "\n",
    "### 8. Run inference\n",
    "\n",
    "Finally, let's try out our models on some sample text. We will use the same sample text for both models to see how they perform on the same input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive inference\n",
    "\n",
    "def run_inference_on_text(text, lora_model, feature_model, tokenizer, device, id2label):\n",
    "    \"\"\"\n",
    "    Run inference on user-provided text with both models\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text for classification\n",
    "    lora_model: Trained LoRA model\n",
    "    feature_model: Trained Feature Extraction model\n",
    "    tokenizer: Tokenizer for preprocessing\n",
    "    device: Device to run inference on\n",
    "    id2label (dict): Mapping from label ids to human-readable labels\n",
    "    \"\"\"\n",
    "    # Move models to eval mode\n",
    "    lora_model.eval()\n",
    "    feature_model.eval()\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    # Run inference with LoRA model\n",
    "    with torch.no_grad():\n",
    "        lora_outputs = lora_model(**inputs)\n",
    "        fe_outputs = feature_model(**inputs)\n",
    "\n",
    "        # Get predictions\n",
    "        lora_logits = lora_outputs.logits\n",
    "        fe_logits = fe_outputs.logits\n",
    "\n",
    "        lora_probs = torch.nn.functional.softmax(lora_logits, dim=-1).cpu().numpy()[0]\n",
    "        fe_probs = torch.nn.functional.softmax(fe_logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "        lora_pred_id = lora_logits.argmax(dim=-1).cpu().numpy()[0]\n",
    "        fe_pred_id = fe_logits.argmax(dim=-1).cpu().numpy()[0]\n",
    "\n",
    "        lora_pred_label = id2label[lora_pred_id]\n",
    "        fe_pred_label = id2label[fe_pred_id]\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f'Input Text: \"{text}\"')\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nPredictions:\")\n",
    "    print(\n",
    "        f\"  LoRA Model: {lora_pred_label} (confidence: {lora_probs[lora_pred_id]:.4f})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Feature Extraction Model: {fe_pred_label} (confidence: {fe_probs[fe_pred_id]:.4f})\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nConfidence Distribution:\")\n",
    "    print(f\"{'Class':<20} {'LoRA':<10} {'Feature Extraction':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, label in id2label.items():\n",
    "        print(f\"{label:<20} {lora_probs[i]:.4f}    {fe_probs[i]:.4f}\")\n",
    "\n",
    "    # Create a bar chart comparing prediction probabilities\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(id2label))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width / 2, lora_probs, width, label=\"LoRA\")\n",
    "    plt.bar(x + width / 2, fe_probs, width, label=\"Feature Extraction\")\n",
    "\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(\"Prediction Probabilities\")\n",
    "    plt.xticks(x, [id2label[i] for i in range(len(id2label))])\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"lora\": {\"label\": lora_pred_label, \"probs\": lora_probs},\n",
    "        \"feature_extraction\": {\"label\": fe_pred_label, \"probs\": fe_probs},\n",
    "    }\n",
    "\n",
    "\n",
    "# Interactive demo function\n",
    "def interactive_demo(lora_model, feature_model, tokenizer, device, id2label):\n",
    "    \"\"\"Run an interactive demo that lets users enter text and see predictions\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Interactive Model Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Enter text to classify (or 'quit' to exit):\")\n",
    "\n",
    "    while True:\n",
    "        text = input(\"\\nYour text: \")\n",
    "        if text.lower() == \"quit\":\n",
    "            print(\"Exiting demo...\")\n",
    "            break\n",
    "        if not text.strip():\n",
    "            print(\"Please enter some text to classify.\")\n",
    "            continue\n",
    "\n",
    "        run_inference_on_text(\n",
    "            text, lora_model, feature_model, tokenizer, device, id2label\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Run the inference\n",
    "\n",
    "We've broken out the inference process into a separate function to keep the code clean and organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a one-time prediction, uncomment the two lines below.\n",
    "sample_text = \"I love cereal.\"\n",
    "run_inference_on_text(sample_text, lora_model, feature_extractor_model, tokenizer, device, id2label)\n",
    "\n",
    "# Example interactive usage.\n",
    "# interactive_demo(lora_model, feature_extractor_model, tokenizer, device, id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Conclusion\n",
    "\n",
    "In this notebook, we implemented two transfer learning techniques for adapting a pre-trained Transformer model to a text classification task. We compared Feature Extraction and LoRA in terms of implementation complexity, performance metrics, and the number of parameters trained.\n",
    "\n",
    "We observed that both techniques can achieve good performance, but they have different trade-offs in terms of efficiency and flexibility. Feature Extraction is simpler to implement and requires fewer resources, while LoRA offers more flexibility and can achieve better performance with fewer trainable parameters.\n",
    "\n",
    "### Bonus Exercise\n",
    "If you want to take this notebook a step further, try the following:\n",
    "* Experiment with different pre-trained models from Hugging Face's model hub. Try using a larger model like `bert-base-uncased` or a more specialized model like `roberta-base`.\n",
    "* Explore other NLP tasks, such as Named Entity Recognition (NER) or Question Answering (QA), using the same techniques.\n",
    "* Try using different datasets from the Hugging Face datasets library. You can find a wide variety of datasets for different NLP tasks [here](https://huggingface.co/datasets)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC-PyTorch-2.3",
   "language": "python",
   "name": "ngc-pytorch-2.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
