{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src=\"https://github.com/PracticumAI/practicumai.github.io/blob/84b04be083ca02e5c7e92850f9afd391fc48ae2a/images/icons/practicumai_computer_vision.png?raw=true\" alt=\"Practicum AI: Computer Vision icon\" align=\"right\" width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Implementation\n",
    "\n",
    "Welcome back! In our previous exercise [01.0_transfer_learning_conepts.ipynb](01.0_transfer_learning_conepts.ipynb), we introduced the core concepts of **Transfer Learning** and experimented with fine-tuning. We saw how leveraging pre-trained models can significantly boost performance and reduce training time compared to starting from scratch.\n",
    "\n",
    "In this notebook we'll expand into adapting powerful pre-trained language models for specific Natural Language Processing (NLP) tasks, focusing on two other, important transfer learning strategies. We'll:\n",
    "\n",
    "* Implement **Feature Extraction** using a pre-trained Transformer model for text classification.\n",
    "* Look closer at how to freeze the base model and train only a new classification head.\n",
    "* Implement **LoRA (Low-Rank Adaptation)**, a Parameter-Efficient Fine-Tuning (PEFT) technique, to adapt the same pre-trained Transformer.\n",
    "* Understand how LoRA modifies the model and drastically reduces the number of trainable parameters.\n",
    "* Directly compare the results and efficiency of Feature Extraction versus LoRA on the same NLP task.\n",
    "\n",
    "## A Direct Comparison\n",
    "\n",
    "To make the comparison between Feature Extraction and LoRA as clear as possible, we will:\n",
    "\n",
    "1.  Use the **same base pre-trained model** [DistilBERT - `distilbert-base-uncased`](https://huggingface.co/distilbert/distilbert-base-uncased) as the starting point for both methods.\n",
    "2.  Apply both techniques to the **same text classification task** (e.g., Text Classification using the [Crop Market News Classification](https://www.kaggle.com/datasets/mcwemzy/crop-market-news-classification)). This dataset is hosted on the fantasic model zoo site, [Kaggle.com](kaggle.com)!\n",
    "3.  Use the **same dataset** for training and evaluation in both parts.\n",
    "\n",
    "This setup will allow us to directly observe the differences in implementation complexity, performance metrics, and the number of parameters trained.\n",
    "\n",
    "### Prerequisites and Setup\n",
    "\n",
    "* **Conceptual Understanding:** Ensure you're comfortable with the basic ideas of transfer learning, pre-trained models, and fine-tuning as covered in Notebook `01.0`.\n",
    "* **Helper Functions:** We will utilize helper functions defined in [00.5_transfer_learning_helper.ipynb](00.5_transfer_learning_helper.ipynb). Please make sure you have access to that notebook or have run it previously to make the functions available.\n",
    "* **Deep Learning Fundamentals:** As with the previous exercise a throrough understanding of how Large Language Models (LLMs) work is not required, but it is necessary to have a basic understanding of concepts like parameters, hyperparameters, and other Deep Learning fundamentals. If you do want to learn more about how LLMs work, we recommend NVidia's [Deep Learning Institute](https://developer.nvidia.com/dli) course on [Introduction to Transformer-Based Natural Language Processing](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-08+V1). This course is free and provides a solid foundation in the principles of Transformers and their applications in NLP.\n",
    "\n",
    "## How to Use This Notebook (`FIX_ME`s)\n",
    "\n",
    "In this notebook, you'll find sections marked with:\n",
    "\n",
    "```\n",
    "FIX_ME\n",
    "# FIX_ME: <description of what to do>\n",
    "```\n",
    "These are places where you need to fill in missing code or make adjustments. The goal is to reinforce the key implementation steps for each technique.\n",
    "\n",
    "If you get stuck, review the preceding explanations, check the documentation for the libraries used (PyTorch, Hugging Face), or consult the course's **GitHub Discussions page** [Link to GitHub Discussions - Placeholder] for hints and help from peers and instructors.\n",
    "\n",
    "Let's get started with adapting our language model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the libraries we will use\n",
    "\n",
    "As always, we will start by importing the libraries we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install peft\n",
    "!pip install liac-arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import datasets\n",
    "import peft\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import pandas as pd\n",
    "import arff\n",
    "from tqdm import tqdm\n",
    "from scipy.io import arff as scipy_arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for GPU availability\n",
    "\n",
    "This cell will check that everything is configured correctly to use your GPU. If everything is correct, you should see something like: \n",
    "\n",
    "    Using GPU: type of GPU\n",
    "\n",
    "If you see:\n",
    "    \n",
    "    Using CPU\n",
    "    \n",
    "Either you do not have a GPU or the kernel is not correctly configured to use it. You might be able to run this notebook, but some sections will take a loooooong time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting the data\n",
    "\n",
    "Today's dataset is the [Crop Market News Classification](https://www.kaggle.com/datasets/mcwemzy/crop-market-news-classification) dataset. This dataset contains crop market news articles and their corresponding categories. The goal is to classify the articles into their respective categories using both Feature Extraction and LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset, extract it to the data folder and remove the zip file\n",
    "download_path = \"https://data.rc.ufl.edu/pub/practicum-ai/Transfer_Learning_Intermediate/crop_market_news.zip\"\n",
    "zip_path = \"data/crop_market_news.zip\"\n",
    "data_path = \"data\"\n",
    "\n",
    "# Paths to dataset - UPDATED to use the correct filename\n",
    "dataset = os.path.join(data_path, \"Crop.Market.News.Classification.arff\")\n",
    "\n",
    "# Check if the data is already loaded\n",
    "if not (os.path.exists(dataset) and os.path.getsize(dataset) > 0):\n",
    "    # Create the data directory if it does not exist\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # Download the zip file\n",
    "    r = requests.get(download_path)\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "\n",
    "    # Remove the zip file\n",
    "    os.remove(zip_path)\n",
    "    print(f\"Data has been downloaded an unziped at {data_path}\")\n",
    "else:\n",
    "    print(\"Data is already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Looking at the data\n",
    "\n",
    "The dataset contains a number of columns, but we will only use the `text` and `label` columns. The `text` column contains the news articles, and the `label` column contains the corresponding categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Check if the dataset file exists and is not empty\n",
    "try:\n",
    "    # Use liac-arff to load the ARFF file\n",
    "    with open(dataset, \"r\") as f:\n",
    "        arff_data = arff.load(f)\n",
    "        data = pd.DataFrame(\n",
    "            arff_data[\"data\"], columns=[attr[0] for attr in arff_data[\"attributes\"]]\n",
    "        )\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset file not found at {dataset}.\")\n",
    "    print(\"Please ensure you have downloaded the dataset and the path is correct.\")\n",
    "    data = None  # Set data to None if file not found\n",
    "\n",
    "if data is not None:\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Loaded DataFrame shape: {df.shape}\")\n",
    "\n",
    "    # Decode byte strings (Common in ARFF)\n",
    "    # Identify potential text columns (adjust names based on actual columns in meta/df.info())\n",
    "    text_col = \"text\"\n",
    "    label_col = \"category\"\n",
    "\n",
    "    if text_col in df.columns and df[text_col].dtype == \"object\":\n",
    "        # Check if decoding is needed (inspect first element)\n",
    "        if isinstance(df[text_col].iloc[0], bytes):\n",
    "            print(f\"Decoding byte strings in column '{text_col}'...\")\n",
    "            df[text_col] = df[text_col].str.decode(\"utf-8\")\n",
    "\n",
    "    if label_col in df.columns and df[label_col].dtype == \"object\":\n",
    "        if isinstance(df[label_col].iloc[0], bytes):\n",
    "            print(f\"Decoding byte strings in column '{label_col}'...\")\n",
    "            df[label_col] = df[label_col].str.decode(\"utf-8\")\n",
    "\n",
    "    # Map String Labels to Integer IDs\n",
    "    unique_labels = df[label_col].unique()\n",
    "    num_labels = len(unique_labels)\n",
    "\n",
    "    # Create mappings\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "    # Apply mapping to create a new 'label' column\n",
    "    df[\"label\"] = df[label_col].map(label2id)\n",
    "\n",
    "    print(f\"Number of classes: {num_labels}\")\n",
    "    print(\"Label mappings created:\")\n",
    "    print(f\"  label2id: {label2id}\")\n",
    "    print(f\"  id2label: {id2label}\")\n",
    "\n",
    "    # Inspect the DataFrame\n",
    "    print(\"\\nDataFrame Head:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    df.info()\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    print(df[\"label\"].value_counts())\n",
    "\n",
    "    # Keep only relevant columns (text and integer label)\n",
    "    df = df[[text_col, \"label\"]]\n",
    "    df = df.rename(columns={text_col: \"text\"})  # Ensure text column is named 'text'\n",
    "else:\n",
    "    print(\"Skipping DataFrame processing as data was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preprocessing the data\n",
    "\n",
    "Now, let's convert our Pandas DataFrame into the Hugging Face `datasets` format, which integrates seamlessly with the `transformers` library. Since the original dataset doesn't specify train/test splits, we'll create our own train and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None:\n",
    "    # Convert Pandas DataFrame to Hugging Face Dataset\n",
    "    hf_dataset = datasets.Dataset.from_pandas(df)\n",
    "    print(\"\\nConverted to Hugging Face Dataset:\")\n",
    "    print(hf_dataset)\n",
    "\n",
    "    # Split into training and validation sets (e.g., 80% train, 20% validation)\n",
    "    train_test_split_ratio = 0.20\n",
    "    dataset_dict = hf_dataset.train_test_split(\n",
    "        test_size=train_test_split_ratio, shuffle=True, seed=42\n",
    "    )  # Use seed for reproducibility\n",
    "\n",
    "    # Rename for clarity\n",
    "    train_ds = dataset_dict[\"train\"]\n",
    "    eval_ds = dataset_dict[\"test\"]\n",
    "\n",
    "    print(\"\\nSplit into Train and Validation Sets:\")\n",
    "    print(f\"  Training examples: {len(train_ds)}\")\n",
    "    print(f\"  Validation examples: {len(eval_ds)}\")\n",
    "    print(train_ds)  # Show structure\n",
    "else:\n",
    "    print(\"Skipping Dataset preparation as data was not loaded.\")\n",
    "    train_ds, eval_ds = None, None  # Set to None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Set up the model and tokenizer\n",
    "\n",
    "We will use the `distilbert-base-uncased` model and tokenizer from Hugging Face. This model is a smaller, faster, and lighter version of BERT *(Bidirectional Encoder Representations from Transformers)* that retains 97% of BERT's language understanding while being 60% faster and 40% smaller. It is a great choice for text classification tasks, especially when computational resources are limited. The `distilbert-base-uncased` model is pre-trained on a large corpus of English text and is designed to handle uncased text (i.e., it does not differentiate between uppercase and lowercase letters), making it ideal for our task. We will also set up the training arguments for both Feature Extraction and LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up models for LoRA and Feature Extraction\n",
    "\n",
    "# Initialize tokenizer\n",
    "model_name = \"bert-base-uncased\"  # Change to your preferred model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- LoRA Setup ---\n",
    "# Load base model for LoRA\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Adjust based on your classification task\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of the update matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"query\", \"key\", \"value\"],  # Which modules to apply LoRA to\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,  # For sequence classification tasks\n",
    ")\n",
    "\n",
    "# Create LoRA model\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "print(\n",
    "    f\"LoRA model setup complete. Trainable parameters: {sum(p.numel() for p in lora_model.parameters() if p.requires_grad)}\"\n",
    ")\n",
    "\n",
    "# --- Feature Extraction Setup ---\n",
    "# Load the base model for feature extraction\n",
    "feature_extractor_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Adjust based on your classification task\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in feature_extractor_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze just the classification head for fine-tuning\n",
    "for param in feature_extractor_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Create dataloaders\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds, shuffle=True, batch_size=16, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_ds, batch_size=16, collate_fn=data_collator)\n",
    "\n",
    "print(\n",
    "    f\"Feature extraction model setup complete. Trainable parameters: {sum(p.numel() for p in feature_extractor_model.parameters() if p.requires_grad)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training the models\n",
    "\n",
    "We will train two models: one using Feature Extraction and the other using LoRA. We will use the same training arguments for both models to ensure a fair comparison. The main difference will be in the training process: \n",
    "A. For Feature Extraction, we will freeze the base model and train only the classification head.\n",
    "B. For LoRA, we will use the LoRA technique to adapt the entire model. \n",
    "\n",
    "We will evaluate both models on the validation set and compare their performance in the section 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            references.extend(labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(references, predictions)\n",
    "    f1 = f1_score(references, predictions, average=\"weighted\")\n",
    "\n",
    "    return {\"loss\": total_loss / len(dataloader), \"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, eval_dataloader, num_epochs=3, device=\"cuda\"):\n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=5e-5)\n",
    "\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    results = {\"train_loss\": [], \"eval_loss\": [], \"eval_accuracy\": [], \"eval_f1\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Training\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        # Evaluation\n",
    "        eval_metrics = evaluate(model, eval_dataloader, device)\n",
    "        results[\"eval_loss\"].append(eval_metrics[\"loss\"])\n",
    "        results[\"eval_accuracy\"].append(eval_metrics[\"accuracy\"])\n",
    "        results[\"eval_f1\"].append(eval_metrics[\"f1\"])\n",
    "\n",
    "        print(\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            + f\"Eval Loss: {eval_metrics['loss']:.4f} | \"\n",
    "            + f\"Accuracy: {eval_metrics['accuracy']:.4f} | \"\n",
    "            + f\"F1 Score: {eval_metrics['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA training\n",
    "lora_results = train_model(\n",
    "    lora_model, train_dataloader, eval_dataloader, num_epochs=3, device=device\n",
    ")\n",
    "\n",
    "# Feature extraction training\n",
    "feature_extraction_results = train_model(\n",
    "    feature_extractor_model,\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    num_epochs=3,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
