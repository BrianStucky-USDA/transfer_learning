{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src=\"https://github.com/PracticumAI/practicumai.github.io/blob/84b04be083ca02e5c7e92850f9afd391fc48ae2a/images/icons/practicumai_computer_vision.png?raw=true\" alt=\"Practicum AI: Computer Vision icon\" align=\"right\" width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Helper\n",
    "\n",
    "Training the models we're going to be looking at in this course can be *very* time consuming. To make the course more manageable, we've separated the training of the models from the rest of the course. Think of it like a cooking show: we'll show you how to make the dish, but we won't make you wait for it to bake in the oven!\n",
    "\n",
    "Below you'll find the logic to train each of the models we use in this course. When you have time, you can experiment with the code and train the models yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "First, let's import the libraries we'll need. These models are trained primarily using the `torchvision` library, which is a part of PyTorch. We'll also use `torch`, `torchvision`, and `torchsummary` to help us train the models.\n",
    "\n",
    "PyTorch is a popular open-source machine learning library for Python, and is developed by Facebook's AI Research lab (FAIR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Import Computer Vision Libraries\n",
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check for GPU availability, and if not available, use CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Transer Learning Concepts - Helper\n",
    "\n",
    "The first models we'll train will be a CNN model trained on our curated Agrinet set, and then VGG19 fine-tuned on the same set. We'll use the `torchvision` library to load the models and datasets, and `torch` to train the models.\n",
    "\n",
    "### 1.1 Load the Data\n",
    "\n",
    "First, we'll download, unpack and load the data. The data is unpacked into the `data` directory, with the training, validation and test sets loaded into `agri_net_train`, `agri_net_val` and `agri_net_test` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset, extract it to the data folder and remove the zip file\n",
    "download_path = \"https://data.rc.ufl.edu/pub/practicum-ai/Transfer_Learning_Intermediate/agrinet_curated.zip\"\n",
    "zip_path = \"data/agrinet_curated.zip\"\n",
    "data_path = \"data\"\n",
    "\n",
    "# Paths to dataset\n",
    "train_dir = os.path.join(data_path, \"agri_net_train\")\n",
    "val_dir = os.path.join(data_path, \"agri_net_val\")\n",
    "test_dir = os.path.join(data_path, \"agri_net_test\")\n",
    "\n",
    "# Check if the data is already loaded\n",
    "if not (\n",
    "    os.path.exists(train_dir) and os.path.exists(val_dir) and os.path.exists(test_dir)\n",
    "):\n",
    "    # Create the data directory if it does not exist\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # Download the zip file\n",
    "    r = requests.get(download_path)\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "\n",
    "    # Remove the zip file\n",
    "    os.remove(zip_path)\n",
    "else:\n",
    "    print(\"Data is already loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create the Data Loaders\n",
    "\n",
    "Next, we'll create the data loaders for the training, validation and test sets. We'll use the `DataLoader` class from `torch.utils.data` to create the data loaders, and the `transforms` module from `torchvision` to apply transformations to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch data transforms\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"test\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Load PyTorch datasets\n",
    "image_datasets = {\n",
    "    \"train\": datasets.ImageFolder(train_dir, data_transforms[\"train\"]),\n",
    "    \"val\": datasets.ImageFolder(val_dir, data_transforms[\"val\"]),\n",
    "    \"test\": datasets.ImageFolder(test_dir, data_transforms[\"test\"]),\n",
    "}\n",
    "\n",
    "# Create PyTorch data loaders\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(\n",
    "        image_datasets[\"train\"],\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "    ),\n",
    "    \"val\": torch.utils.data.DataLoader(\n",
    "        image_datasets[\"val\"],\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "    ),\n",
    "    \"test\": torch.utils.data.DataLoader(\n",
    "        image_datasets[\"test\"],\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define the Model\n",
    "\n",
    "We'll define the model we're going to train. The first model we'll train is a CNN model, which is a simple convolutional neural network. We'll define the layers of the model using the `nn` module from `torch`.\n",
    "\n",
    "üìù **Note:**\n",
    "If you'd like more information on how CNNs work, we explored them as part of Deep Learning Foundations (DLF) course, and have a full Computer Vision Intermediate course. The final notebook of the DLF course, `DLF_01.1_bees_vs_wasps.ipynb`, is included in this repository if you'd like to review the material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "# Define baseline model using PyTorch\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 112 * 112, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "num_classes = len(image_datasets[\"train\"].classes)\n",
    "baseline_model_pt = BaselineModel(num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(baseline_model_pt.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Define early stopping parameters\n",
    "early_stopping_patience = 3\n",
    "best_loss = float(\"inf\")\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Train the CNN Model\n",
    "\n",
    "Next we'll train the CNN. We'll use the `train()` method to train the model. The `train()` method takes the model, the data loaders, the loss function, the optimizer, and the number of epochs as arguments, and trains the model for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the baseline model using PyTorch\n",
    "num_epochs = 5  # Number of epochs to train. Increase this value for better results\n",
    "for epoch in range(num_epochs):\n",
    "    baseline_model_pt.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm.tqdm(  # tqdm is used to show the progress of the training\n",
    "        dataloaders[\"train\"], desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False\n",
    "    )\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = baseline_model_pt(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / len(image_datasets[\"train\"])\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check. If the loss does not improve for 'early_stopping_patience' epochs, stop training\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Save the CNN Model\n",
    "\n",
    "Finally, we'll save the trained model to a file. We'll use the `torch.save()` method to save the model to a file, and the `torch.load()` method to load the model from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to save the models if it does not exist\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# Save the trained CNN model\n",
    "torch.save(baseline_model_pt.state_dict(), \"models/baseline_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Load the VGG19 Model and Define our Hyperparameters\n",
    "\n",
    "Per the paper [\"The Power of Transfer Learning for Agricultural Applications: Agrinet\"](https://arxiv.org/abs/2207.03881), the best result the research team was able to achieve used an ImageNet pre-trained VGG19 model, and was fine-tuned on the Agrinet dataset. VGG19 (Visual Geometry Group, with 19 layers) is a specific computer vision model, unlike the very generic CNN we just trained as a baseline. We'll load the pre-trained VGG19 model from `torchvision.models`, and fine-tune it on the Agrinet dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG19 model pre-trained on ImageNet dataset\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "\n",
    "# Freeze the pre-trained layers\n",
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier to match the number of classes in the dataset (their are 86 classes in the dataset!)\n",
    "num_features = vgg19.classifier[6].in_features\n",
    "vgg19.classifier[6] = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Move the model to the device (GPU if available)\n",
    "vgg19 = vgg19.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg19.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Define early stopping parameters\n",
    "early_stopping_patience = 3\n",
    "best_loss = float(\"inf\")\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Train the VGG19 Model\n",
    "\n",
    "Next we'll train the VGG19 model. We'll use the `train()` method to train the model. The `train()` method takes the model, the data loaders, the loss function, the optimizer, and the number of epochs as arguments, and trains the model for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VGG19 model using PyTorch\n",
    "num_epochs = 5  # Number of epochs to train. Increase this value for better results\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vgg19.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm.tqdm(  # tqdm is used to show the progress of the training\n",
    "        dataloaders[\"train\"], desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False\n",
    "    )\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vgg19(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / len(image_datasets[\"train\"])\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check. If the loss does not improve for 'early_stopping_patience' epochs, stop training\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Save the VGG19 Model\n",
    "\n",
    "Finally, we'll save the trained model to a file. We'll use the `torch.save()` method to save the model to a file, and the `torch.load()` method to load the model from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to save the models if it does not exist\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# Save the fine-tuned VGG19 model\n",
    "torch.save(vgg19.state_dict(), \"models/vgg19_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Transfer Learning Concepts - Helper: Conclusion\n",
    "\n",
    "That's it! We've trained a CNN model and a VGG19 model on the Agrinet dataset. We've saved the models to files, and we can now use them to make predictions on new images. For evaluations and predictions, please see the `01.0_Transfer_Learning_Concepts.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
